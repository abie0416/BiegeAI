import os
import logging
import sys

# Set protobuf environment variable BEFORE any other imports
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from agent.gemini_client import GeminiClient
from agent.mcp import run_mcp

try:
    from services.llamaindex_graphrag_service import get_llamaindex_graphrag_service
except ImportError:
    # Fallback for Railway deployment
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from services.llamaindex_graphrag_service import get_llamaindex_graphrag_service
from conversation_manager import conversation_manager
from utils.environment import log_environment_info, get_environment_info

# Configure logging with forced output
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Suppress verbose LlamaIndex debug logs
logging.getLogger("llama_index.core.node_parser.node_utils").setLevel(logging.WARNING)
logging.getLogger("llama_index.core.indices.knowledge_graph.base").setLevel(logging.WARNING)
logging.getLogger("llama_index.core.indices.vector_store.base").setLevel(logging.WARNING)
logging.getLogger("llama_index.core.retrievers").setLevel(logging.WARNING)
logging.getLogger("llama_index.core.query_engine").setLevel(logging.WARNING)

# Force stdout to flush immediately
sys.stdout.flush()

# Load environment variables
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Debug startup information
logger.info("üöÄ Starting Biege AI Backend...")
logger.info(f"üìã Environment check:")
logger.info(f"   - GEMINI_API_KEY: {'‚úÖ Set' if GEMINI_API_KEY else '‚ùå Not set'}")
log_environment_info()

app = FastAPI(title="Biege AI Backend", description="AI Agent with RAG and MCP capabilities")

# Allow CORS for local frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Gemini client
try:
    gemini_client = GeminiClient(GEMINI_API_KEY)
    logger.info("‚úÖ Gemini client initialized successfully")
except Exception as e:
    logger.error(f"‚ùå Failed to initialize Gemini client: {e}")
    gemini_client = None

def get_documents_from_sheets_with_fallback():
    """Get documents from Google Sheets with fallback to hardcoded documents"""
    fallback_documents = [
        {"content": "EricÊòØOÂ≠ùÂ≠êÔºå‰ªñÈùûÂ∏∏Â≠ùÈ°∫Áà∂ÊØçÔºåÁªèÂ∏∏Â∏ÆÂä©ÂÆ∂ÈáåÂÅöÂÆ∂Âä°„ÄÇEricÂú®ÊúãÂèã‰∏≠ÂæàÂèóÊ¨¢ËøéÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢Âíå‰ªñ‰∏ÄËµ∑Áé©„ÄÇ"},
        {"content": "EricÊäïÁØÆËøòÂèØ‰ª•Ôºå‰ΩÜÊòØÊ≤°ÊúâzznÂáÜ„ÄÇEricÂú®ÁØÆÁêÉÂú∫‰∏äË°®Áé∞‰∏çÈîôÔºå‰ΩÜÊòØzznÁöÑÊäïÁØÆÊäÄÊúØÊõ¥Âä†Á≤æÂáÜÔºåÂëΩ‰∏≠ÁéáÊõ¥È´ò„ÄÇ"},
        {"content": "911ÊØî718ÁâõÈÄºÔºå‰øùÊó∂Êç∑911ÊòØÁªèÂÖ∏Ë∑ëËΩ¶ÔºåÊÄßËÉΩÂçìË∂ä„ÄÇ718ËôΩÁÑ∂‰πü‰∏çÈîôÔºå‰ΩÜÂú®ÂæàÂ§öÊñπÈù¢ÈÉΩ‰∏çÂ¶Ç911Âá∫Ëâ≤„ÄÇ"},
        {"content": "È©¨Ê£öÊòØËÄÅÂè∏Êú∫ÔºåÂºÄËΩ¶ÊäÄÊúØÂæàÂ•ΩÔºåÁªèÈ™å‰∏∞ÂØå„ÄÇ‰ªñÁªèÂ∏∏ÂºÄËΩ¶Â∏¶Â§ßÂÆ∂Âá∫ÂéªÁé©ÔºåÂ§ßÂÆ∂ÈÉΩËßâÂæóÂùê‰ªñÁöÑËΩ¶ÂæàÂÆâÂÖ®„ÄÇ"},
        {"content": "FinalÂÖ®ËÉΩÁéãÔºåÂú®Ê∏∏ÊàèFinal‰∏≠Ë°®Áé∞ÈùûÂ∏∏Âá∫Ëâ≤ÔºåÂêÑÁßçËßíËâ≤ÈÉΩËÉΩÁé©ÂæóÂæàÂ•Ω„ÄÇ‰ªñÊòØÂ§ßÂÆ∂ÂÖ¨ËÆ§ÁöÑÊ∏∏ÊàèÈ´òÊâã„ÄÇ"},
        {"content": "Â∞èÁò¶Âì•Áé©È∏üÁãôÂæàÂéâÂÆ≥ÔºåÂú®Â∞ÑÂáªÊ∏∏Êàè‰∏≠ÁâπÂà´ÊìÖÈïø‰ΩøÁî®ÁãôÂáªÊû™„ÄÇ‰ªñÁöÑÁûÑÂáÜÊäÄÊúØÈùûÂ∏∏Á≤æÂáÜÔºåÁªèÂ∏∏ËÉΩ‰∏ÄÊû™ÁàÜÂ§¥„ÄÇ"},
        {"content": "ÊÆµÁ•ûÊòØÈÅìÂÖ∑ÁéãÔºåÂú®Ê∏∏Êàè‰∏≠ÁâπÂà´ÊìÖÈïø‰ΩøÁî®ÂêÑÁßçÈÅìÂÖ∑„ÄÇ‰ªñÂØπÊ∏∏ÊàèÈÅìÂÖ∑ÁöÑÁêÜËß£ÂæàÊ∑±ÔºåÊÄªËÉΩÊâæÂà∞ÊúÄÊúâÊïàÁöÑ‰ΩøÁî®ÊñπÊ≥ï„ÄÇ"},
        {"content": "Âê¥ÈùûÊòØ‰∫öÈ©¨ÈÄäAIÁéãÔºåÂú®‰∫öÈ©¨ÈÄäÂ∑•‰ΩúÔºå‰∏ìÈó®Ë¥üË¥£AIÁõ∏ÂÖ≥È°πÁõÆ„ÄÇ‰ªñÂú®‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÂæàÊúâÁªèÈ™åÔºåÊäÄÊúØËÉΩÂäõÂæàÂº∫„ÄÇ"},
        {"content": "abieÂ∞±ÊòØÊÜãÂì•ÔºåËøôÊòØ‰ªñÁöÑÂ§ñÂè∑„ÄÇabieÊÄßÊ†ºÊØîËæÉÂÜÖÂêëÔºå‰∏çÂ§™Áà±ËØ¥ËØùÔºåÊâÄ‰ª•Â§ßÂÆ∂Âè´‰ªñÊÜãÂì•„ÄÇ"},
        {"content": "ÊÜãÂì•ÁâõÈÄºÔºåËôΩÁÑ∂abieÊØîËæÉÂÜÖÂêëÔºå‰ΩÜÊòØ‰ªñÁöÑËÉΩÂäõÂæàÂº∫ÔºåÂú®ÂæàÂ§öÊñπÈù¢ÈÉΩÂæàÂá∫Ëâ≤ÔºåÂ§ßÂÆ∂ÈÉΩÂæà‰Ω©Êúç‰ªñ„ÄÇ"},
    ]
    
    try:
        from services.google_sheets import sheets_service
        sheets_docs = sheets_service.get_documents()
        if sheets_docs:
            logger.info(f"‚úÖ Successfully fetched {len(sheets_docs)} documents from Google Sheets")
            return sheets_docs
        else:
            logger.warning("‚ö†Ô∏è No documents found in Google Sheets, using fallback data")
            return fallback_documents
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not fetch from Google Sheets: {e}, using fallback data")
        return fallback_documents

def initialize_rag_knowledge_base():
    """Initialize RAG knowledge base on startup"""
    try:
        logger.info("üöÄ Initializing RAG knowledge base...")
        
        if not GEMINI_API_KEY:
            logger.warning("‚ö†Ô∏è GEMINI_API_KEY not set, skipping RAG initialization")
            return False
        
        # Get GCP configuration from environment
        gcp_bucket_name = os.getenv("GCP_BUCKET_NAME")
        gcp_project_id = os.getenv("GCP_PROJECT_ID")
        
        # Initialize LlamaIndex GraphRAG with GCP support
        try:
            llamaindex_graphrag = get_llamaindex_graphrag_service(
                google_api_key=GEMINI_API_KEY,
                gcp_bucket_name=gcp_bucket_name,
                gcp_project_id=gcp_project_id
            )
            
            # Try to initialize from GCP first
            if gcp_bucket_name and gcp_project_id:
                logger.info("üîÑ Attempting to initialize RAG from GCP...")
                if llamaindex_graphrag.initialize_from_gcp():
                    logger.info("‚úÖ RAG knowledge base initialized from GCP successfully")
                    return True
                else:
                    logger.warning("‚ö†Ô∏è Failed to initialize from GCP, trying local storage...")
            
            # Try to load from local storage
            if llamaindex_graphrag.load_index():
                logger.info("‚úÖ RAG knowledge base loaded from local storage")
                return True
            
            # If no existing index found, don't rebuild automatically
            logger.warning("‚ö†Ô∏è No existing RAG index found in GCP or local storage")
            logger.info("üí° Use the /rebuild-rag endpoint to build the initial knowledge base")
            return False
                
        except Exception as e:
            logger.error(f"‚ùå LlamaIndex GraphRAG initialization failed: {e}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error initializing RAG knowledge base: {e}")
        return False

# Initialize RAG knowledge base on startup
rag_initialized = initialize_rag_knowledge_base()

def get_rag_context(question: str) -> str:
    """Fetch RAG context using GraphRAG"""
    try:
        logger.info("üîç Fetching RAG context with GraphRAG...")
        
        if not GEMINI_API_KEY:
            logger.warning("‚ö†Ô∏è GEMINI_API_KEY not set, cannot use RAG")
            return "RAG not available - API key not configured."
        
        # Use LlamaIndex GraphRAG
        try:
            llamaindex_graphrag = get_llamaindex_graphrag_service(GEMINI_API_KEY)
            documents = llamaindex_graphrag.hybrid_search(question, k=5)
            
            if documents:
                # Format documents into context
                context_parts = []
                logger.info(f"üìö Formatting {len(documents)} retrieved documents into context:")
                
                for i, doc in enumerate(documents):
                    source_info = f" ({doc['source']})" if 'source' in doc else ""
                    score_info = f" (relevance: {doc['score']:.3f})" if doc['score'] is not None else ""
                    context_parts.append(f"Document {i+1}{source_info}{score_info}:\n{doc['content']}")
                    
                    # Log each document being added to context
                    logger.info(f"   üìÑ Document {i+1}:")
                    logger.info(f"      - Source: {doc.get('source', 'unknown')}")
                    logger.info(f"      - Score: {doc.get('score', 'N/A')}")
                    logger.info(f"      - Content length: {len(doc['content'])} characters")
                    logger.info(f"      - Content preview: {doc['content'][:200]}...")
                
                context = "\n\n".join(context_parts)
                logger.info(f"‚úÖ GraphRAG context fetched: {len(context)} characters from {len(documents)} documents")
                logger.info(f"üìã Final context preview: {context[:300]}...")
                return context
        except Exception as e:
            logger.error(f"‚ùå LlamaIndex GraphRAG failed: {e}")
        
        logger.warning("‚ö†Ô∏è No documents retrieved from RAG system")
        return "No relevant documents found in knowledge base."
        
    except Exception as e:
        logger.error(f"‚ùå RAG context fetch failed: {e}")
        return f"RAG Error: {str(e)}"

@app.get("/")
async def root():
    logger.info("üì• Root endpoint accessed")
    return {
        "message": "Welcome to Biege AI Backend!",
        "status": "running",
        "debug": {
            "api_key_configured": GEMINI_API_KEY is not None,
            "gemini_client_ready": gemini_client is not None,
            "rag_initialized": rag_initialized,
            "port": os.getenv("PORT", "Not set"),
            "environment": "production" if os.getenv("RAILWAY_ENVIRONMENT") else "development"
        },
        "endpoints": {
            "health": "/health",
            "ask": "/ask (POST)"
        }
    }

@app.get("/test-logs")
async def test_logs():
    """Test endpoint to verify logging is working"""
    logger.debug("DEBUG: This is a debug message")
    logger.info("INFO: This is an info message")
    logger.warning("WARNING: This is a warning message")
    logger.error("ERROR: This is an error message")
    
    # Also test print statements
    print("PRINT: This is a print statement")
    sys.stdout.flush()
    
    return {"message": "Log test completed", "check_console": True}

@app.post("/ask")
async def ask(request: Request):
    logger.info("üì• /ask endpoint accessed")
    
    # Check if Gemini client is available
    if not gemini_client:
        error_msg = "‚ùå Gemini client not initialized. Check GEMINI_API_KEY configuration."
        logger.error(error_msg)
        return {"answer": error_msg, "debug": {"error": "gemini_client_not_initialized"}}
    
    try:
        data = await request.json()
        question = data.get("question")
        session_id = data.get("session_id")  # Optional session ID from frontend
        
        if not question:
            logger.warning("‚ö†Ô∏è No question provided in request")
            return {"answer": "Please provide a question.", "debug": {"error": "no_question_provided"}}
        
        logger.info(f"ü§î Processing question: {question[:50]}...")
        
        # Step 1: Manage conversation session
        session_id = conversation_manager.get_or_create_session(session_id)
        logger.info(f"üìù Using conversation session: {session_id}")
        
        # Step 2: Get conversation context
        conversation_context, context_debug = conversation_manager.get_conversation_context(session_id, question)
        logger.info(f"üí¨ Conversation context: {context_debug['message_count']} messages, {context_debug['context_length']} chars")
        
        # Step 3: Always fetch RAG context
        rag_context = get_rag_context(question)
        
        # Step 4: Combine RAG and conversation context
        combined_context = ""
        if conversation_context:
            combined_context += f"Previous conversation:\n{conversation_context}\n\n"
        if rag_context:
            combined_context += f"Relevant knowledge:\n{rag_context}\n\n"
        
        # Log the combined context being sent to the model
        logger.info(f"üì§ Sending combined context to model:")
        logger.info(f"   - Question: {question}")
        logger.info(f"   - Conversation context length: {len(conversation_context)} characters")
        logger.info(f"   - RAG context length: {len(rag_context)} characters")
        logger.info(f"   - Combined context length: {len(combined_context)} characters")
        logger.info(f"   - Combined context preview: {combined_context[:500]}...")
        
        # Step 5: Run MCP with combined context
        try:
            answer = run_mcp(question, gemini_client, combined_context)
            logger.info(f"‚úÖ MCP completed successfully")
            method_used = "MCP_WITH_COMBINED_CONTEXT"
        except Exception as e:
            logger.error(f"‚ùå MCP failed: {e}")
            # Fallback to direct RAG answer if MCP fails
            answer = rag_context
            method_used = "RAG_FALLBACK"
            logger.info("‚ö†Ô∏è Using RAG answer as fallback")
        
        # Step 6: Store messages in conversation history
        conversation_manager.add_message(session_id, "user", question)
        conversation_manager.add_message(session_id, "agent", answer)
        
        # Step 7: Prepare debug information
        debug_info = {
            "session_id": session_id,
            "rag_context_length": len(rag_context),
            "conversation_context_length": context_debug['context_length'],
            "combined_context_length": len(combined_context),
            "final_method": method_used,
            "question_length": len(question),
            "answer_length": len(answer) if answer else 0,
            "conversation_stats": context_debug
        }
        
        logger.info(f"‚úÖ Successfully processed question using {method_used}")
        return {
            "answer": answer,
            "session_id": session_id,
            "debug": debug_info
        }
        
    except Exception as e:
        error_msg = f"‚ùå Unexpected error: {str(e)}"
        logger.error(f"‚ùå Error in /ask endpoint: {e}")
        return {
            "answer": error_msg, 
            "debug": {
                "error": "unexpected_error",
                "error_type": type(e).__name__,
                "error_message": str(e)
            }
        }

@app.get("/health")
async def health_check():
    logger.info("üì• Health check accessed")
    return {
        "status": "healthy", 
        "message": "AI Agent is running",
        "debug": {
            "api_key_configured": GEMINI_API_KEY is not None,
            "gemini_client_ready": gemini_client is not None,
            "rag_initialized": rag_initialized,
            "port": os.getenv("PORT", "Not set"),
            "environment": "production" if os.getenv("RAILWAY_ENVIRONMENT") else "development"
        }
    }

@app.get("/debug")
async def debug_info():
    """Debug endpoint to check service status"""
    logger.info("üì• Debug endpoint accessed")
    return {
        "service": "Biege AI Backend",
        "status": "running",
        "environment_variables": {
            "GEMINI_API_KEY": "Set" if GEMINI_API_KEY else "Not set",
            "PORT": os.getenv("PORT", "Not set"),
            "RAILWAY_ENVIRONMENT": os.getenv("RAILWAY_ENVIRONMENT", "Not set")
        },
        "services": {
            "gemini_client": "Ready" if gemini_client else "Not ready",
            "rag_knowledge_base": "Initialized" if rag_initialized else "Not initialized - use /rebuild-rag to build"
        },
        "conversation_manager": {
            "total_sessions": len(conversation_manager.sessions),
            "max_sessions": conversation_manager.max_sessions,
            "session_timeout_minutes": conversation_manager.session_timeout_minutes
        },
        "environment": get_environment_info(),
        "endpoints": {
            "root": "/",
            "health": "/health",
            "ask": "/ask (POST)",
            "debug": "/debug",
            "test_logs": "/test-logs",
            "conversations": "/conversations (GET)",
            "conversation_stats": "/conversation-stats (GET)",
            "rebuild_rag": "/rebuild-rag (POST) - rebuilds Advanced RAG knowledge base"
        }
    }

@app.get("/conversations")
async def get_conversations():
    """Get all conversation sessions"""
    logger.info("üì• Conversations endpoint accessed")
    return conversation_manager.get_all_sessions_stats()

@app.get("/conversation-stats")
async def get_conversation_stats():
    """Get conversation manager statistics"""
    logger.info("üì• Conversation stats endpoint accessed")
    return {
        "total_sessions": len(conversation_manager.sessions),
        "max_sessions": conversation_manager.max_sessions,
        "session_timeout_minutes": conversation_manager.session_timeout_minutes,
        "max_messages_per_session": conversation_manager.max_messages_per_session,
        "max_context_length": conversation_manager.max_context_length,
        "consecutive_timeout_minutes": conversation_manager.consecutive_timeout_minutes
    }

@app.post("/rebuild-rag")
async def rebuild_rag():
    """Rebuild RAG knowledge base (both Advanced RAG and GraphRAG) and return results"""
    logger.info("üì• Rebuild RAG endpoint accessed")
    
    try:
        # Ensure environment variables are loaded
        from dotenv import load_dotenv
        load_dotenv()
        
        # Get documents from Google Sheets with fallback
        documents = get_documents_from_sheets_with_fallback()

        
        total_documents = len(documents)
        
        # Initialize results
        results = {
            "llamaindex_graphrag": {"success": False, "nodes_created": 0, "edges_created": 0}
        }
        

        

        
        # Rebuild LlamaIndex GraphRAG
        logger.info("üî® Rebuilding LlamaIndex GraphRAG knowledge base...")
        try:
            if not GEMINI_API_KEY:
                logger.warning("‚ö†Ô∏è GEMINI_API_KEY not set, skipping LlamaIndex GraphRAG rebuild")
                results["llamaindex_graphrag"] = {"success": False, "nodes_created": 0, "edges_created": 0, "error": "GEMINI_API_KEY not configured"}
            else:
                # Get GCP configuration
                gcp_bucket_name = os.getenv("GCP_BUCKET_NAME")
                gcp_project_id = os.getenv("GCP_PROJECT_ID")
                
                # Get LlamaIndex GraphRAG service with GCP support
                llamaindex_graphrag = get_llamaindex_graphrag_service(
                    google_api_key=GEMINI_API_KEY,
                    gcp_bucket_name=gcp_bucket_name,
                    gcp_project_id=gcp_project_id
                )
                
                # Build knowledge graph
                success = llamaindex_graphrag.build_knowledge_graph(documents)
                
                if success:
                    # Save to both local storage and GCP
                    save_success = llamaindex_graphrag.save_index()
                    
                    # Get graph statistics
                    stats = llamaindex_graphrag.get_graph_statistics()
                    results["llamaindex_graphrag"] = {
                        "success": True, 
                        "nodes_created": stats.get("total_nodes", 0),
                        "edges_created": stats.get("total_edges", 0),
                        "node_types": stats.get("node_types", {}),
                        "relationship_types": stats.get("relationship_types", {}),
                        "saved_locally": save_success,
                        "saved_to_gcp": gcp_bucket_name is not None and gcp_project_id is not None
                    }
                    logger.info(f"‚úÖ LlamaIndex GraphRAG rebuild completed successfully. {stats.get('total_nodes', 0)} nodes, {stats.get('total_edges', 0)} edges.")
                    if gcp_bucket_name:
                        logger.info(f"üì§ Index saved to GCP bucket: {gcp_bucket_name}")
                else:
                    results["llamaindex_graphrag"] = {"success": False, "nodes_created": 0, "edges_created": 0, "error": "LlamaIndex GraphRAG build process failed"}
                    
        except Exception as e:
            logger.error(f"‚ùå Error rebuilding LlamaIndex GraphRAG: {e}")
            results["llamaindex_graphrag"] = {"success": False, "nodes_created": 0, "edges_created": 0, "error": str(e)}
        
        # Update global RAG status
        global rag_initialized
        rag_initialized = results["llamaindex_graphrag"]["success"]
        
        # Prepare response
        llamaindex_success = results["llamaindex_graphrag"]["success"]
        
        # Create detailed message
        details_parts = []
        if llamaindex_success:
            details_parts.append(f"LlamaIndex GraphRAG: {results['llamaindex_graphrag']['nodes_created']} nodes, {results['llamaindex_graphrag']['edges_created']} edges")
        
        if details_parts:
            details = f"Successfully rebuilt: {'; '.join(details_parts)} from {total_documents} source documents"
        else:
            details = "RAG rebuild failed"
        
        return {
            "success": llamaindex_success,
            "message": f"RAG knowledge base rebuild {'completed' if llamaindex_success else 'failed'}",
            "total_sample_documents": total_documents,
            "details": details,
            "llamaindex_graphrag": results["llamaindex_graphrag"]
        }
            
    except Exception as e:
        error_msg = f"‚ùå Error rebuilding RAG knowledge base: {str(e)}"
        logger.error(f"‚ùå Error in rebuild_rag endpoint: {e}")
        return {
            "success": False,
            "message": error_msg,
            "total_sample_documents": "unknown",
            "error": str(e)
        }

@app.get("/llamaindex-graph-stats")
async def get_llamaindex_graph_stats():
    """Get LlamaIndex GraphRAG knowledge graph statistics"""
    logger.info("üì• LlamaIndex Graph stats endpoint accessed")
    
    try:
        if not GEMINI_API_KEY:
            return {"error": "GEMINI_API_KEY not configured"}
        
        llamaindex_graphrag = get_llamaindex_graphrag_service(GEMINI_API_KEY)
        stats = llamaindex_graphrag.get_graph_statistics()
        
        return {
            "success": True,
            "graph_statistics": stats,
            "message": f"LlamaIndex knowledge graph has {stats.get('total_nodes', 0)} nodes and {stats.get('total_edges', 0)} edges"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting LlamaIndex graph stats: {e}")
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/llamaindex-graph-search")
async def llamaindex_graph_search(request: Request):
    """Perform LlamaIndex GraphRAG search"""
    logger.info("üì• LlamaIndex Graph search endpoint accessed")
    
    try:
        data = await request.json()
        query = data.get("query")
        k = data.get("k", 5)
        
        if not query:
            return {"error": "Query parameter is required"}
        
        if not GEMINI_API_KEY:
            return {"error": "GEMINI_API_KEY not configured"}
        
        llamaindex_graphrag = get_llamaindex_graphrag_service(GEMINI_API_KEY)
        results = llamaindex_graphrag.hybrid_search(query, k=k)
        
        return {
            "success": True,
            "query": query,
            "results": results,
            "count": len(results)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error in LlamaIndex graph search: {e}")
        return {
            "success": False,
            "error": str(e)
        }



 